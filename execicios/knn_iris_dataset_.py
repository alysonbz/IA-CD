# -*- coding: utf-8 -*-
"""knn iris dataset .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wgt3HInpQoLDU9xnlfC1hGNg_4TJTvAM
"""

lista = []
with open('iris.data', 'r') as f:
    for linha in f:
        if linha.strip() != '':
            a = linha.strip().split(',')
            # converte as 4 primeiras colunas para float
            features = [float(x) for x in a[:-1]]
            # mapeia a classe para número
            classe_str = a[-1]
            if classe_str == 'Iris-setosa':
                classe = 1.0
            elif classe_str == 'Iris-versicolor':
                classe = 2.0
            elif classe_str == 'Iris-virginica':
                classe = 3.0
            else:
                continue  # ignora linhas inválidas (se houver)
            lista.append(features + [classe])

print(lista)
def countclasses(lista):
    setosa=0
    versicolor=0
    virginica=0
    for i in range(len(lista)):
        if lista[i][4] == 1.0:
            setosa += 1
        if lista[i][4] == 2.0:
            versicolor += 1
        if lista[i][4] == 3.0:
            virginica += 1

    return [setosa,versicolor,virginica]

p=0.6
print(lista)
setosa,versicolor, virginica = countclasses(lista)
treinamento, teste= [], []
max_setosa, max_versicolor, max_virginica = int(p*setosa), int(p*versicolor), int(p*virginica)
total1 =0
total2 =0
total3 =0
for lis in lista:
    if lis[-1]==1.0 and total1< max_setosa:
        treinamento.append(lis)
        total1 +=1
    elif lis[-1]==2.0 and total2<max_versicolor:
        treinamento.append(lis)
        total2 +=1
    elif lis[-1]==3.0 and total3<max_virginica:
        treinamento.append(lis)
        total3 +=1
    else:
        teste.append(lis)

import math
def dist_euclidiana(v1,v2):
    dim, soma = len(v1), 0
    for i in range(dim -1):
        soma += math.pow(v1[i] -v2[i],2)
    return math.sqrt(soma)

def dist_manhattan(v1, v2):
    dim, soma = len(v1), 0
    for i in range(dim -1):
        soma += abs(v1[i] - v2[i])
    return math.sqrt(soma)

def dist_minkowski(v1, v2, p):
    soma = 0
    for i in range(len(v1)-1):
        soma += abs(v1[i] - v2[i]) ** p
    return soma ** (1 / p)

def dist_chebyshev(v1, v2):
    max_diff = 0
    for i in range(len(v1)-1):
        diff = abs(v1[i] - v2[i])
        if diff > max_diff:
            max_diff = diff
    return max_diff

def knn(treinamento, nova_amostra, K, distancia_func, **kwargs):
    dists = {}
    len_treino = len(treinamento)

    for i in range(len_treino):
        # Passa argumentos extras se necessário (por exemplo, p para Minkowski)
        d = distancia_func(treinamento[i], nova_amostra, **kwargs) if kwargs else distancia_func(treinamento[i], nova_amostra)
        dists[i] = d

    k_vizinhos = sorted(dists, key=dists.get)[:K]

    qtd_setosa, qtd_versicolor, qtd_virginica = 0, 0, 0
    for indice in k_vizinhos:
        if treinamento[indice][-1] == 1.0:
            qtd_setosa += 1
        elif treinamento[indice][-1] == 2.0:
            qtd_versicolor += 1
        else:
            qtd_virginica += 1

    a = [qtd_setosa, qtd_versicolor, qtd_virginica]
    return a.index(max(a)) + 1.0

acertos, K = 0, 1
for amostra in teste:
    classe = knn(treinamento, amostra, K, dist_euclidiana)
    if amostra[-1] == classe:
        acertos += 1
print("Porcentagem de acertos (Euclidiana):", 100 * acertos / len(teste))

acertos = 0
for amostra in teste:
    classe = knn(treinamento, amostra, K, dist_manhattan)
    if amostra[-1] == classe:
        acertos += 1
print("Porcentagem de acertos (Manhattan):", 100 * acertos / len(teste))

acertos = 0
for amostra in teste:
    classe = knn(treinamento, amostra, K, dist_manhattan)
    if amostra[-1] == classe:
        acertos += 1
print("Porcentagem de acertos (Manhattan):", 100 * acertos / len(teste))

acertos = 0
for amostra in teste:
    classe = knn(treinamento, amostra, K, dist_chebyshev)
    if amostra[-1] == classe:
        acertos += 1
print("Porcentagem de acertos (Chebyshev):", 100 * acertos / len(teste))